{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-]","pipeline":["stemmer"]},"docs":[{"location":"","title":"\u5199\u200b\u5728\u200b\u524d\u9762","text":"<p>NLP\u200b\u7b97\u6cd5\u200b\u5de5\u7a0b\u5e08\u200b\u5b66\u4e60\u200b\u7b14\u8bb0\u200b</p>"},{"location":"#_2","title":"\u642d\u5efa\u200b\u5de5\u5177","text":"<p>\u200b\u672c\u200bNotebook\u200b\u4f7f\u7528\u200bMaterial for MkDocs\u200b\u642d\u5efa\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u540c\u65f6\u200b\u64b0\u5199\u200bdocs\u200b\u548c\u200bblog\u200b\u4e24\u79cd\u200b\u98ce\u683c\u200b\u7684\u200b\u5b66\u4e60\u200b\u7b14\u8bb0\u200b\u3002</p>"},{"location":"algorithms/","title":"\u5f15\u8a00","text":""},{"location":"algorithms/#_2","title":"\u5b66\u4e60\u200b\u5efa\u8bae","text":"<p>\u200b\u591a\u591a\u200b\u9605\u8bfb\u200b\u5f00\u6e90\u200b\u793e\u533a\u200b\u7684\u200b\u9898\u89e3\u200b\uff0c\u200b\u5b66\u4e60\u200b\u4f18\u79c0\u200b\u7684\u200b\u4ee3\u7801\u200b\u98ce\u683c\u200b\u3001\u200b\u547d\u540d\u200b\u65b9\u6cd5\u200b\u7b49\u7b49\u200b\u3002</p> <p>\u200b\u6ce8\u91cd\u200b\u66f4\u4f18\u200b\u89e3\u6cd5\u200b\uff0c\u200b\u4e0d\u8981\u200b\u4e3a\u4e86\u200b\u6253\u5361\u200b\u800c\u5237\u9898\u200b\uff0c\u200b\u91cd\u5728\u200b\u6536\u83b7\u200b\u3002</p>"},{"location":"algorithms/#_3","title":"\u5b66\u4e60\u200b\u5185\u5bb9","text":"<p>\u200b\u5177\u4f53\u200b\u5b66\u4e60\u200b\u5185\u5bb9\u200b\u53ef\u200b\u53c2\u8003\u200bNOI\u200b\u5927\u7eb2\u200b\u548c\u200boi-wiki</p>"},{"location":"algorithms/codeforces_problem_list/","title":"Codeforces\u200b\u9898\u5355","text":"<p>CF1878B - Aleksa and Stack (1)</p> <ol> <li>\u200b\u5947\u6570\u200b\u5e8f\u5217\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4e86\u200b\uff0c\u200b\u4e0d\u8981\u200b\u628a\u200b\u95ee\u9898\u200b\u5904\u7406\u200b\u5f97\u200b\u5f88\u200b\u590d\u6742\u200b</li> </ol> <p>CF1870D - Prefix Purchase</p> <p>CF1872F - Selling a Menagerie (1)</p> <ol> <li>\u200b\u8003\u5bdf\u200b\u56fe\u200b\u76f8\u5173\u200b\u95ee\u9898\u200b\u7684\u200b\u5206\u6790\u200b\u89e3\u51b3\u200b\u80fd\u529b\u200b</li> </ol> <p>CF1873H - Mad City (1)</p> <ol> <li>\u200b\u5148\u5bf9\u200bb\u200b\u505a\u200b\u6df1\u5ea6\u200b\u4f18\u5148\u200b\u641c\u7d22\u200b\u627e\u5230\u200b\u8fdb\u5165\u200b\u73af\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u518d\u200b\u8ba1\u7b97\u200b\u8fd9\u4e2a\u200b\u4f4d\u7f6e\u200b\u5230\u200ba\u200b\u548c\u200bb\u200b\u7684\u200b\u8ddd\u79bb\u200b\uff0c\u200b\u5982\u679c\u200b\u79bb\u200bb\u200b\u66f4\u8fd1\u200b\u5219\u200bb\u200b\u6c38\u8fdc\u200b\u65e0\u6cd5\u200b\u88ab\u200b\u6293\u200b\u5230\u200b\u3002\u200b\u53c2\u8003\u200b\u9898\u89e3\u200b\u3002</li> </ol> <p>CF1817A - Almost Increasing Subsequence (1)</p> <ol> <li>\u200b\u5148\u200b\u627e\u5230\u200b\u7ed3\u679c\u200b\u7684\u200b\u4e0a\u754c\u200b\uff0c\u200b\u9a8c\u8bc1\u200b\u5176\u200b\u53ef\u53d6\u200b\u5230\u200b\uff0c\u200b\u518d\u200b\u9009\u62e9\u200b\u5408\u9002\u200b\u7684\u200b\u6570\u636e\u7ed3\u6784\u200b\u5b9e\u73b0\u200b</li> </ol> <p></p> <p>CF1914D - Three Activities</p> <p>CF1864C - Divisor Chain (1)</p> <ol> <li>\\(x=2^n*l\\)\uff0c\u200b\u5148\u200b\u4e00\u6b65\u6b65\u200b\u5c06\u200b\\(l\\)\u200b\u51cf\u4e00\u200b\uff0c\u200b\u5e76\u200b\u5265\u79bb\u200b\u56e0\u5b50\u200b2\uff0c\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200b2\u200b\u7684\u200b\u5e42\u6b21\u200b\uff0c\u200b\u518d\u200b\u4f9d\u6b21\u200b\u51cf\u53bb\u200b2\u200b\u5e42\u6b21\u200b\u7684\u200b\u4e00\u534a\u200b\u6700\u540e\u200b\u5f97\u5230\u200b1</li> </ol> <p>CF1873G - ABBC or BACB (1)</p> <ol> <li>\u200b\u8003\u5bdf\u200b\u57fa\u672c\u200b\u7684\u200b\u4ece\u200b\u7279\u6b8a\u200b\u5230\u200b\u4e00\u822c\u200b\u7684\u200b\u95ee\u9898\u200b\u5206\u6790\u200b\u80fd\u529b\u200b</li> </ol> <p>CF1879B - Chips on the Board (1)</p> <ol> <li>\u200b\u53ef\u4ee5\u200b\u8bc1\u660e\u200b\u7b54\u6848\u200b\u4e3a\u200b\uff1a\\(min(sum(a)+n*min(b), sum(b)+n*min(a))\\)</li> </ol>"},{"location":"algorithms/leetcode_problem_list/","title":"LeetCode\u200b\u9898\u5355","text":"<p>LeetCode 236 - \u200b\u4e8c\u53c9\u6811\u200b\u7684\u200b\u6700\u8fd1\u200b\u516c\u5171\u200b\u7956\u5148\u200b</p> <p>LeetCode 765 - \u200b\u60c5\u4fa3\u200b\u7275\u624b\u200b (1)</p> <ol> <li>\u200b\u5199\u200b\u8d77\u6765\u200b\u5f88\u200b\u7b80\u5355\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7ed9\u200b\u522b\u4eba\u200b\u8bb2\u8bb2\u200b\u8fd9\u9898\u200b\u4e3a\u4ec0\u4e48\u200b\u7b80\u5355\u200b\u7684\u200b\u5199\u6cd5\u200b\u5c31\u662f\u200b\u5bf9\u200b\u7684\u200b</li> </ol> <p>LeetCode 2003 - \u200b\u6bcf\u200b\u68f5\u5b50\u200b\u6811\u5185\u200b\u7f3a\u5931\u200b\u7684\u200b\u6700\u5c0f\u200b\u57fa\u56e0\u200b\u503c\u200b</p> <p>LeetCode 1296 - \u200b\u5212\u5206\u200b\u6570\u7ec4\u200b\u4e3a\u200b\u8fde\u7eed\u200b\u6570\u5b57\u200b\u7684\u200b\u96c6\u5408\u200b (1)</p> <ol> <li>\u200b\u4e24\u5c42\u200b\u5faa\u73af\u200b\u7684\u200b\u590d\u6742\u5ea6\u200b\u5e76\u200b\u4e0d\u662f\u200b\\(n*k\\)</li> </ol>"},{"location":"blog/","title":"\u535a\u5ba2\u200b\u5217\u8868","text":"<p>\u200b\u5305\u62ec\u200b\u4e00\u4e9b\u200b\u6280\u672f\u200b\u7684\u200b\u8c03\u7814\u200b\uff0c\u200b\u516c\u5f00\u8bfe\u200b\u5b66\u4e60\u200b\u7b14\u8bb0\u200b\uff0c\u200b\u6709\u8da3\u200b\u7684\u200b\u9879\u76ee\u200b\u7b49\u7b49\u200b\uff0c\u200b\u4e5f\u200b\u5305\u542b\u200b\u4e00\u4e9b\u200b\u751f\u6d3b\u200b\u7b14\u8bb0\u200b\u3002</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","title":"Llama\u200b\u6e90\u7801\u200b\u9605\u8bfb","text":"<p>Llama<sup>1</sup>\u200b\u662f\u200b\u7531\u200bMeta\u200b\u8bbe\u8ba1\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u5e76\u200b\u5f00\u6e90\u200b\u7684\u200b\u5927\u200b\u8bed\u8a00\u200b\u6a21\u578b\u200b\u3002\u200b\u76f8\u6bd4\u200b\u4e8e\u200bGPT-3\uff0cLlama\u200b\u6a21\u578b\u200b\u66f4\u200b\u5c0f\u200b\uff0c\u200b\u4f46\u662f\u200b\u8bad\u7ec3\u200b\u66f4\u52a0\u200b\u5145\u5206\u200b\uff0c\u200b\u6027\u80fd\u200b\u66f4\u5f3a\u200b\uff0c\u200b\u662f\u200b\u5f00\u6e90\u200b\u793e\u533a\u200b\u6700\u200b\u53d7\u6b22\u8fce\u200b\u7684\u200b\u5927\u200b\u6a21\u578b\u200b\u4e4b\u4e00\u200b\u3002</p> <p>\u200b\u672c\u6587\u200b\u4e3b\u8981\u200b\u9605\u8bfb\u200bHuggingface\u200b\u7684\u200bLlama\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u91cd\u70b9\u200b\u5173\u6ce8\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u6700\u65e9\u200b\u7684\u200bTransformer<sup>3</sup>\uff0cLlama\u200b\u91c7\u7528\u200b\u4e86\u200b\u54ea\u4e9b\u200b\u65b0\u200b\u7684\u200b\u6280\u672f\u200b\u548c\u200b\u4f18\u5316\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u8fd9\u4efd\u200b\u4ee3\u7801\u200b\u4e5f\u200b\u517c\u5bb9\u200bLlama2<sup>2</sup>\u200b\u7684\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u5177\u4f53\u8868\u73b0\u200b\u5728\u200bgrouped query attention\u200b\u7684\u200b\u5b9e\u73b0\u200b\u4e0a\u200b\u3002</p> <p>\u200b\u8df3\u8f6c\u200b\u8fd9\u91cc\u200b\u76f4\u63a5\u200b\u5f00\u59cb\u200b\u6e90\u7801\u200b\u9605\u8bfb\u200b\u3002</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#_1","title":"\u9884\u5907\u200b\u77e5\u8bc6","text":""},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#transformer","title":"Transformer","text":"<p>\u200b\u4f60\u200b\u9700\u8981\u200b\u77e5\u9053\u200b\u4ec0\u4e48\u200b\u662f\u200bTransformer\uff0c\u200b\u77e5\u9053\u200b\u5b83\u200b\u662f\u200b\u4e00\u79cd\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002</p> <p></p> <p>\u200b\u539f\u59cb\u200b\u7684\u200bTransformer\u200b\u7f51\u7edc\u7ed3\u6784\u200b<sup>3</sup></p> <p>\u200b\u5f3a\u70c8\u63a8\u8350\u200b\u9605\u8bfb\u200b\u539f\u200b\u8bba\u6587\u200b\uff0c\u200b\u91cd\u70b9\u200b\u5173\u6ce8\u200b3.2\u200b\u8282\u200b\u548c\u200b3.3\u200b\u8282\u5bf9\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u7684\u200b\u63cf\u8ff0\u200b\u3002\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u5982\u679c\u200b\u5728\u200b\u9605\u8bfb\u200b\u4e86\u200b\u539f\u200b\u8bba\u6587\u200b\u4e4b\u540e\u200b\u4f60\u200b\u8fd8\u662f\u200b\u4e0d\u200b\u786e\u5b9a\u200b\u5b83\u200b\u7684\u200b\u5b9e\u73b0\u200b\u4e5f\u200b\u6ca1\u5173\u7cfb\u200b\uff0c\u200b\u901a\u8fc7\u200b\u9605\u8bfb\u200bLlama\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u77e5\u9053\u200b\u4e00\u4e2a\u200b\u57fa\u4e8e\u200bTransformer\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u7684\u200b\u8bed\u8a00\u200b\u6a21\u578b\u200b\u662f\u200b\u5982\u4f55\u200b\u5b9e\u73b0\u200b\u7684\u200b\u3002</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#pytorch","title":"Pytorch","text":"<p>\u200b\u4f60\u200b\u9700\u8981\u200b\u77e5\u9053\u200b\u57fa\u672c\u200b\u7684\u200bpytorch\u200b\u77e5\u8bc6\u200b\uff0c\u200b\u77e5\u9053\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u7528\u4e8e\u200b\u642d\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u7f51\u4e0a\u200b\u627e\u200b\u4e00\u4e9b\u200b\u6700\u200b\u57fa\u672c\u200b\u7684\u200bpytorch\u200b\u6559\u7a0b\u200b\uff0c\u200b\u53ea\u8981\u200b\u4f60\u200b\u80fd\u770b\u61c2\u200b\u4e0b\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4e86\u200b\u3002</p> simple_network.py<pre><code>import torch\nfrom torch import nn\n\n# \u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\nclass NeuralNetwork(nn.Module):\n    def __init__(self, in_dim=3, hidden_dim=6, out_dim=2):\n        super().__init__()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, out_dim),\n        )\n\n    def forward(self, x: torch.Tensor):\n        return self.linear_relu_stack(x)\n\n# \u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\nnet = NeuralNetwork()\ninput_tensor = torch.randn(10, 3)\noutput_tensor = net(input_tensor)\n</code></pre>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#_2","title":"\u4ee3\u7801\u200b\u51c6\u5907","text":"<p>\u200b\u672c\u6587\u200b\u6240\u200b\u9605\u8bfb\u200b\u7684\u200b\u4ee3\u7801\u200b\u4ee5\u200b\u4e0b\u9762\u200b\u7684\u200b\u7248\u672c\u200b\u4e3a\u51c6\u200b\uff1a</p> requirements.txt<pre><code>torch==2.0.1\ntransformers==4.31.0\n</code></pre> <p>transformers\u200b\u5e93\u200b\u91c7\u7528\u200b\u5355\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\u7b56\u7565\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u9605\u8bfb\u200b<code>modeling_llama.py</code>\u200b\u5373\u53ef\u200b\u3002</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#_3","title":"\u6e90\u7801\u200b\u9605\u8bfb","text":""},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#_4","title":"\u4e86\u89e3\u200b\u4ee3\u7801\u200b\u7ed3\u6784","text":"<p>\u200b\u5728\u200b\u9605\u8bfb\u200b\u5177\u4f53\u200b\u7684\u200b\u5b9e\u73b0\u200b\u524d\u200b\uff0c\u200b\u5e94\u5f53\u200b\u5bf9\u200b\u4ee3\u7801\u200b\u7684\u200b\u6574\u4f53\u200b\u7ed3\u6784\u200b\u903b\u8f91\u200b\u6709\u6240\u200b\u4e86\u89e3\u200b\u3002\u200b\u8fd9\u200b\u4e00\u90e8\u5206\u200b\u4e0d\u200b\u9700\u8981\u200b\u641e\u6e05\u695a\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u7ec6\u8282\u200b\uff0c\u200b\u4f46\u662f\u200b\u9700\u8981\u200b\u4e86\u89e3\u200b\u6a21\u578b\u200b\u7684\u200b\u5b9e\u73b0\u200b\u4ee3\u7801\u200b\u662f\u200b\u5982\u4f55\u200b\u7ec4\u7ec7\u200b\u7684\u200b\u3002</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#_5","title":"\u9605\u8bfb\u200b\u5927\u7eb2","text":"<p>\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u89c2\u5bdf\u200b\u4e00\u4e0b\u200b<code>modeling_llama.py</code>\u200b\u6587\u4ef6\u200b\u91cc\u200b\u6709\u200b\u54ea\u4e9b\u200b\u7c7b\u200b\u548c\u200b\u51fd\u6570\u200b\uff0c\u200b\u5728\u200bvscode\u200b\u4e2d\u200b\u6253\u5f00\u200b\u5de6\u8fb9\u200b\u7684\u200b\u5927\u7eb2\u200b\u3002</p> <p></p> <p>\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\u5927\u7eb2\u200b</p> <p>\u200b\u4e0d\u96be\u731c\u6d4b\u200b\uff0cLlamaModel\u200b\u7c7b\u200b\u5c31\u662f\u200b\u6211\u4eec\u200b\u8981\u200b\u627e\u200b\u7684\u200b\u6a21\u578b\u200b\u4e3b\u5e72\u200b\uff0c\u200b\u800c\u200bLlamaAttention\u3001LlamaMLP\u200b\u7b49\u200b\u7c7b\u200b\u5219\u200b\u662f\u200b\u6a21\u578b\u200b\u4e2d\u200b\u5177\u4f53\u200b\u7684\u200b\u7f51\u7edc\u200b\u6a21\u5757\u200b\u3002\u200b\u66f4\u8fdb\u4e00\u6b65\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u5bf9\u200bTransformer\u200b\u67b6\u6784\u200b\u6bd4\u8f83\u200b\u719f\u6089\u200b\u7684\u8bdd\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u731c\u6d4b\u200bLlamaDecoderLayer\u200b\u662f\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u7684\u200bTransformer\u200b\u7f51\u7edc\u200b\uff0c\u200b\u5176\u4e2d\u200b\u5305\u542b\u200b\u4e86\u200bLlamaAttention\u200b\u548c\u200bLlamaMLP\u200b\u6a21\u5757\u200b\u3002</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#llamaforcausallm","title":"LlamaForCausalLM\u200b\u7c7b","text":"<p>\u200b\u89c2\u5bdf\u200bLlamaForCausalLM\u200b\u7c7b\u200b</p> modeling_llama.py<pre><code>class LlamaForCausalLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n        self.pretraining_tp = config.pretraining_tp\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u5b83\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200bLlamaModel\u200b\u5bf9\u8c61\u200b\u548c\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u7684\u200blm_head\uff0c\u200b\u540e\u8005\u200b\u7528\u4e8e\u200b\u8ba1\u7b97\u200b\u4e0b\u200b\u4e00\u4e2a\u200btoken\u200b\u7684\u200b\u6982\u7387\u5206\u5e03\u200b\u3002</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#llamamodel","title":"LlamaModel\u200b\u7c7b","text":"<p>\u200b\u89c2\u5bdf\u200bLlamaModel\u200b\u7c7b\u200b</p> modeling_llama.py<pre><code>class LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u5b83\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200bEmbedding\u200b\u5c42\u200b\u3001\u200b\u4e00\u7cfb\u5217\u200b\u7684\u200bLlamaDecoderLayer\u3001\u200b\u548c\u200b\u4e00\u4e2a\u200bLlamaRMSNorm\u200b\u6a21\u5757\u200b\u3002</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#llamadecoderlayer","title":"LlamaDecoderLayer\u200b\u7c7b","text":"<p>\u200b\u89c2\u5bdf\u200bllamaDecoderLayer\u200b\u7c7b\u200b</p> modeling_llama.py<pre><code>class LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LlamaAttention(config=config)\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n</code></pre> <p>\u200b\u5176\u200b\u5305\u62ec\u200b\u4e86\u200bTransformer\u200b\u7ed3\u6784\u200b\u4e2d\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u4e24\u4e2a\u200b\u6a21\u5757\u200b\uff0c\u200b\u5373\u200bself-attention\u200b\u548c\u200bFFN\uff0c\u200b\u5206\u522b\u200b\u662f\u200b\u4e00\u4e2a\u200bLlamaAttention\u200b\u5bf9\u8c61\u200b\u548c\u200bLlamaMLP\u200b\u5bf9\u8c61\u200b\uff0c\u200b\u6b64\u5916\u200b\u8fd8\u6709\u200b\u4e24\u4e2a\u200bLlamaRMSNorm\u200b\u5bf9\u8c61\u200b\u3002</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#_6","title":"\u9605\u8bfb\u200b\u5177\u4f53\u200b\u5b9e\u73b0","text":"<p>\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u6765\u200b\u9605\u8bfb\u200b\u6a21\u578b\u200b\u7684\u200b\u5177\u4f53\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5c06\u200b\u91cd\u70b9\u200b\u653e\u5728\u200bLlama\u200b\u6a21\u578b\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u6700\u65e9\u200b\u7684\u200bTransformer\u200b\u91c7\u7528\u200b\u4e86\u200b\u54ea\u4e9b\u200b\u65b0\u200b\u7684\u200b\u6280\u672f\u200b\u548c\u200b\u4f18\u5316\u200b\u3002</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#rmsnorm","title":"RMSNorm","text":"<p>LayerNorm<sup>4</sup>\u200b\u662f\u200b\u4e00\u79cd\u200b\u7a33\u5b9a\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6280\u672f\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5bf9\u200b\u6fc0\u6d3b\u200b\u503c\u200b\u5411\u91cf\u200b\u8fdb\u884c\u200bnormalize\u200b\u64cd\u4f5c\u200b\u6765\u200b\u7a33\u5b9a\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3002Llama\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200bRMSNorm<sup>5</sup>\uff0c\u200b\u53bb\u6389\u200b\u4e86\u200bre-centering\u200b\u7684\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u6548\u7387\u200b\u9ad8\u4e8e\u200b\u7ecf\u5178\u200b\u7684\u200bLayerNorm\u3002</p> modeling_llama.py<pre><code>class LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n</code></pre>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#swiglu","title":"SwiGLU","text":"<p>\u200b\u5728\u200bTransformer\u200b\u7684\u200bFFN\u200b\u5b9e\u73b0\u200b\u4e2d\u200b\uff0cSwiGLU\u200b\u88ab\u200b\u8bc1\u660e\u200b\u662f\u200b\u6027\u80fd\u200b\u8f83\u200b\u597d\u200b\u4e00\u79cd\u200b\u5b9e\u73b0\u200b<sup>6</sup>\u3002</p> modeling_llama.py<pre><code>class LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.pretraining_tp = config.pretraining_tp\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        if self.pretraining_tp &gt; 1:\n            slice = self.intermediate_size // self.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat([F.linear(x, gate_proj_slices[i]) for i in range(self.pretraining_tp)], dim=-1)\n            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.pretraining_tp)], dim=-1)\n\n            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n            down_proj = [F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.pretraining_tp)]\n            down_proj = sum(down_proj)\n        else:\n            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n        return down_proj\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200bLlama\u200b\u7684\u200bMLP\u200b\u5c42\u200b\u4f7f\u7528\u200b\u4e86\u200bGLU\u200b\u7684\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5373\u200b\u5e26\u6709\u200b\u4e00\u4e2a\u200b\u95e8\u63a7\u200b\u673a\u5236\u200b\u3002</p> <p>\u200b\u5b9e\u9645\u200b\u4e2d\u200b\u4f1a\u200b\u8c03\u6574\u200b\u4e2d\u95f4\u5c42\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u6765\u200b\u4f7f\u5f97\u200b\u53c2\u200b\u6570\u91cf\u200b\u548c\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u4e0e\u200b\u539f\u59cb\u200b\u7684\u200bFFN\u200b\u5b9e\u73b0\u200b\u76f8\u5f53\u200b\uff08\u200b\u5e38\u89c1\u200b\u7684\u200b\u4e2d\u95f4\u5c42\u200b\u7ef4\u5ea6\u200b\u662f\u200bhidden_size\u200b\u7684\u200b\\(\\frac{8}{3}\\)\u200b\u500d\u200b\u5de6\u53f3\u200b\uff0c\u200b\u548c\u200b\u539f\u59cb\u200b4\u200b\u500d\u200b\u5927\u5c0f\u200b\u7684\u200b\u4e2d\u95f4\u5c42\u200b\u7684\u200b\u53c2\u200b\u6570\u91cf\u200b\u548c\u200b\u8ba1\u7b97\u200b\u91cf\u200b\u76f8\u5f53\u200b\uff09\u3002</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#rotary-embedding","title":"Rotary Embedding","text":"<p>\u200b\u7531\u4e8e\u200bTransformer\u200b\u7684\u200bAttention\u200b\u8ba1\u7b97\u200b\u662f\u200b\u4e0d\u200b\u5e26\u6709\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\u7684\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b\u67d0\u79cd\u200b\u65b9\u6cd5\u200b\u8ba9\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u611f\u77e5\u200b\u5230\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u6280\u672f\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u3002\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u7684\u200b\u5b9e\u73b0\u200b\u65b9\u5f0f\u200b\u6709\u200b\u5f88\u200b\u591a\u79cd\u200b\uff0c\u200b\u4ece\u200b\u6700\u65e9\u200b\u7684\u200b\u7edd\u5bf9\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b<sup>3</sup>\uff0c\u200b\u5230\u200b\u76f8\u5bf9\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\uff0c\u200b\u518d\u200b\u5230\u200bLlama\u200b\u4f7f\u7528\u200b\u7684\u200b\u65cb\u8f6c\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b<sup>8</sup>\u3002</p> <p>\u200b\u63d0\u51fa\u200b\u65cb\u8f6c\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b\u7684\u200b\u6587\u7ae0\u200b<sup>8</sup>\u200b\u5728\u200b\u672c\u200bblog\u200b\u64b0\u5199\u200b\u65f6\u5019\u200b\uff0c\u200b\u516c\u5f0f\u200b\u90e8\u5206\u200b\u7684\u200b\u8bb0\u53f7\u200b\u8fd8\u662f\u200b\u7a0d\u5fae\u200b\u6709\u4e9b\u200b\u6df7\u4e71\u200b\uff0c\u200b\u9700\u8981\u200b\u9759\u4e0b\u5fc3\u6765\u200b\u770b\u200b\u660e\u767d\u200b\u3002</p> modeling_llama.py<pre><code>def rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n</code></pre>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#llamaattention","title":"LlamaAttention","text":"<p>LlamaAttention\u200b\u7684\u200b\u4e3b\u8981\u53c2\u6570\u200b\u4e3a\u200b\u56db\u4e2a\u200b\u6620\u5c04\u200b\u77e9\u9635\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5b9e\u73b0\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u3002</p> modeling_llama.py<pre><code>class LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.pretraining_tp = config.pretraining_tp\n        self.max_position_embeddings = config.max_position_embeddings\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self._init_rope()\n</code></pre> <p>\u200b\u8fd9\u91cc\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200bk_proj\u200b\u548c\u200bv_proj\u200b\u5e76\u4e0d\u4e00\u5b9a\u200b\u662f\u200bhidden_size * (num_heads * head_dim)\u200b\u7684\u200b\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200bLlama2\u200b\u4f7f\u7528\u200b\u4e86\u200bgrouped query attention\uff0c\u200b\u5373\u200b\u4e00\u4e9b\u200b\u5934\u200b\u5171\u4eab\u200b\u4e86\u200bk\u200b\u548c\u200bv\u200b\u7684\u200b\u6620\u5c04\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u7684\u200bnum_key_value_heads\u200b\u662f\u200b\u771f\u6b63\u200b\u7684\u200bk\u200b\u548c\u200bv\u200b\u5934\u200b\u7684\u200b\u4e2a\u6570\u200b\u3002\u200b\u5f53\u200bnum_key_value_heads\u200b\u7b49\u4e8e\u200bnum_heads\u200b\u65f6\u200b\uff0c\u200b\u5c31\u662f\u200b\u6700\u200b\u7ecf\u5178\u200b\u7684\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u8d70\u4e00\u904d\u200bAttention\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u8ba1\u7b97\u200b\u8fc7\u7a0b\u200b\u3002\u200b\u9996\u5148\u200b\u5148\u200b\u660e\u786e\u200b\u6bcf\u4e2a\u200bAttention Block\u200b\u7684\u200b\u8f93\u5165\u200b\u7ef4\u5ea6\u200b\u4e3a\u200bbatch_size * seq_len * hidden_size</p> modeling_llama.py<pre><code>    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n    ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n</code></pre> <p>\u200b\u5bf9\u200b\u8f93\u5165\u200b\u8fdb\u884c\u200b\u6620\u5c04\u200b\uff0c\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u4e86\u200bgrouped query attention\uff0c\u200b\u90a3\u4e48\u200bkey_states\u200b\u548c\u200bvalue_states\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ef4\u5ea6\u200b\u4e3a\u200bbatch_size * seq_len * (num_key_value_heads * head_dim)\uff0c\u200b\u5176\u4f59\u200b\u60c5\u51b5\u200b\u4e3a\u200b\u8f93\u51fa\u200b\u7ef4\u5ea6\u200b\u4e3a\u200bbatch_size * seq_len * (num_heads * head_dim)</p> modeling_llama.py<pre><code>            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n</code></pre> <p>\u200b\u62c6\u5206\u200b\u51fa\u200b\u6bcf\u200b\u4e00\u4e2a\u5934\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u6b64\u65f6\u200b\u8f93\u51fa\u200b\u7ef4\u5ea6\u200b\u53d8\u4e3a\u200bbatch_size * num_heads * seq_len * head_dim\u200b\u6216\u200bbatch_size * num_key_value_heads * seq_len * head_dim</p> modeling_llama.py<pre><code>        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n</code></pre> <p>\u200b\u8ba1\u7b97\u200b\u65cb\u8f6c\u200b\u4f4d\u7f6e\u200b\u7f16\u7801\u200b</p> modeling_llama.py<pre><code>        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n</code></pre> <p>\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u4e86\u200bgrouped query attention\uff0c\u200b\u5219\u200b\u62f7\u8d1d\u200bkey_states\u200b\u548c\u200bvalue_states\uff0c\u200b\u6b64\u65f6\u200bkey_states\u200b\u548c\u200bvalue_states\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u53d8\u4e3a\u200bbatch_size * num_heads * seq_len * head_dim</p> modeling_llama.py<pre><code>        # repeat k/v heads if n_kv_heads &lt; n_heads\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n</code></pre> <p>\u200b\u4e0b\u9762\u200b\u662f\u200brepeat_kv\u200b\u7684\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u4f7f\u7528\u200b\u4e86\u200bNone\u200b\u6765\u200b\u589e\u52a0\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u518d\u200b\u4f7f\u7528\u200bexpand\u200b\u62f7\u8d1d\u200b\u4e86\u200b\uff08\u200b\u5176\u5b9e\u200b\u6ca1\u6709\u200b\u771f\u7684\u200b\u62f7\u8d1d\u200b\u5185\u5b58\u200b\uff09\u200b\u90a3\u4e9b\u200b\u5171\u4eab\u200b\u7684\u200b\u5934\u200b\u8f93\u51fa\u200b\u3002</p> modeling_llama.py<pre><code>def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -&gt; torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n</code></pre> <p>\u200b\u8ba1\u7b97\u200battention weights\u200b\u7684\u200blogits\uff0c\u200b\u5176\u200b\u7ef4\u5ea6\u200b\u4e3a\u200bbatch_size * num_heads * seq_len * seq_len</p> modeling_llama.py<pre><code>        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n</code></pre> <p>\u200b\u8ba1\u7b97\u200battention\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u5176\u200b\u7ef4\u5ea6\u200b\u4e3a\u200bbatch_size * num_heads * seq_len * head_dim</p> modeling_llama.py<pre><code>        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n</code></pre> <p>\u200b\u5c06\u200b\u591a\u5934\u200b\u8f93\u51fa\u200bcat\u200b\u5230\u200b\u4e00\u8d77\u200b\uff08\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b356\u200b\u884c\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u4e86\u200bview\u200b\u7684\u8bdd\u200b355\u200b\u884c\u200b\u9700\u8981\u200b\u4f7f\u7528\u200bcontiguous\uff0c\u200b\u5982\u679c\u200b\u4f7f\u7528\u200breshape\u200b\u5176\u5b9e\u200b\u4e0d\u5199\u200bcontiguous\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\uff09\uff0c\u200b\u8f93\u51fa\u200b\u7ef4\u5ea6\u200b\u4e3a\u200bbatch_size * seq_len * hidden_size</p> modeling_llama.py<pre><code>        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n</code></pre> <p>\u200b\u6700\u540e\u200b\u5c06\u200b\u8f93\u51fa\u200b\u901a\u8fc7\u200bo_proj\u200b\u7684\u200b\u6620\u5c04\u200b\uff0c\u200b\u5f97\u5230\u200b\u7ef4\u5ea6\u200b\u4e3a\u200bbatch_size * seq_len * hidden_size</p> modeling_llama.py<pre><code>            attn_output = self.o_proj(attn_output)\n</code></pre>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#pre-layernorm","title":"Pre-LayerNorm","text":"<p>\u200b\u539f\u59cb\u200b\u7684\u200bTransformer\u200b\u4f7f\u7528\u200bpost-layernorm\uff0c\u200b\u7814\u7a76\u200b\u8868\u660e\u200bpre-layernorm\u200b\u4f1a\u200b\u4f7f\u5f97\u200b\u8bad\u7ec3\u200b\u66f4\u52a0\u200b\u7a33\u5b9a\u200b<sup>7</sup>\u3002</p> <p></p> <p>\u200b\u5de6\u4fa7\u200b\u4e3a\u200bpost-layernorm\uff0c\u200b\u53f3\u4fa7\u200b\u4e3a\u200bpre-layernorm<sup>7</sup></p> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200bpre-layernorm\u200b\u662f\u200b\u52a0\u200b\u5728\u200bAttention\u200b\u548c\u200bMLP\u200b\u4e4b\u524d\u200b\u7684\u200b\uff1a</p> modeling_llama.py<pre><code>        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n</code></pre>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#language-modeling-loss","title":"Language modeling loss","text":"<p>\u200b\u5f53\u200b\u8f93\u5165\u200b\u4e86\u200blabels\u200b\u65f6\u200b\uff0cLlamaForCausalLM\u200b\u4f1a\u200b\u5728\u200b\u6bcf\u200b\u4e00\u4e2a\u200btoken\u200b\u4f4d\u7f6e\u200b\u8ba1\u7b97\u200b\u4e0b\u200b\u4e00\u4e2a\u200btoken\u200b\u7684\u200b\u8f93\u51fa\u200b\u635f\u5931\u200b\uff0c\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200blogits\u200b\u548c\u200blabels\u200b\u662f\u200b\u9700\u8981\u200b\u9519\u5f00\u200b\u4e00\u4f4d\u200b\u7684\u200b\u3002</p> modeling_llama.py<pre><code>        loss = None\n        if labels is not None:\n            # Shift so that tokens &lt; n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n</code></pre>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#_7","title":"\u672c\u6587\u200b\u672a\u200b\u8ba8\u8bba\u200b\u7684\u200b\u5185\u5bb9","text":"<p>\u200b\u5927\u200b\u6a21\u578b\u200b\u7684\u200b\u5e76\u884c\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u63a8\u7406\u200b\uff0c\u200b\u4f5c\u4e3a\u200b\u672a\u6765\u200b\u7684\u200b\u5b66\u4e60\u200b\u8ba1\u5212\u200b</p>"},{"location":"blog/2023/09/29/llama%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#_8","title":"\u5ef6\u4f38\u200b\u9605\u8bfb","text":"<p>Andrej Karpathy\u200b\u7684\u200bYoutube\u200b\u9891\u9053\u200b\u4e2d\u6709\u200b\u624b\u200b\u6495\u200bGPT\u200b\u4ee3\u7801\u200b\u7684\u200b\u6559\u7a0b\u200b\uff0c\u200b\u5f3a\u70c8\u63a8\u8350\u200b\u89c2\u770b\u200b\u3002</p> <p>Meta\u200b\u5f00\u6e90\u200b\u7684\u200bllama\u200b\u4ee3\u7801\u200b</p> <ol> <li> <p>Touvron et al. LLaMA: Open and Efficient Foundation Language Models (arXiv 2023)\u00a0\u21a9</p> </li> <li> <p>Touvron et al. Llama 2: Open Foundation and Fine-Tuned Chat Models (arXiv 2023)\u00a0\u21a9</p> </li> <li> <p>Vaswani et al. Attention Is All You Need. (NIPS 2017)\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Ba et al. Layer Normalization (arXiv 2023)\u00a0\u21a9</p> </li> <li> <p>Zhang et al. Root Mean Square Layer Normalization (NIPS 2019)\u00a0\u21a9</p> </li> <li> <p>Shazeer et al. GLU Variants Improve Transformer (arXiv 2020)\u00a0\u21a9</p> </li> <li> <p>Xiong et al. On Layer Normalization in the Transformer Architecture (ICML 2020)\u00a0\u21a9\u21a9</p> </li> <li> <p>Su et al. RoFormer: Enhanced Transformer with Rotary Position Embedding (arXiv 2021)\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/","title":"Markdown Syntax in Material for MkDocs","text":"<p>Markdown\u200b\u662f\u200b\u4e00\u79cd\u200b\u8f7b\u91cf\u5316\u200b\u7684\u200b\u6807\u8bb0\u200b\u8bed\u8a00\u200b\uff0c\u200b\u975e\u5e38\u9002\u5408\u200b\u7528\u4e8e\u200b\u7f16\u5199\u200b\u6280\u672f\u200b\u6587\u6863\u200b\u3002\u200b\u672c\u6587\u200b\u5c55\u793a\u200b\u4e86\u200b\u7528\u4e8e\u200b\u7f16\u5199\u200b\u8fd9\u672c\u200bNotebook\u200b\u7684\u200b\u4e3b\u8981\u200bMarkdown\u200b\u8bed\u6cd5\u200b\u3002</p>"},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_1","title":"\u4e00\u7ea7\u200b\u6807\u9898","text":""},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_2","title":"\u4e8c\u7ea7\u200b\u6807\u9898","text":""},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_3","title":"\u4e09\u7ea7\u200b\u6807\u9898","text":""},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_4","title":"\u56db\u7ea7\u200b\u6807\u9898","text":""},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_5","title":"\u4e94\u7ea7\u200b\u6807\u9898","text":""},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_6","title":"\u516d\u7ea7\u200b\u6807\u9898","text":""},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_7","title":"\u6bb5\u843d","text":"<p>\u200b\u7eaf\u200b\u6587\u672c\u200b</p> <p>\u200b\u53e6\u5916\u200b\u4e00\u4e9b\u200b\u7eaf\u200b\u6587\u672c\u200b</p>"},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_8","title":"\u5f3a\u8c03\u200b\u8bed\u6cd5","text":"<p>\u200b\u7c97\u4f53\u200b</p> <p>\u200b\u659c\u4f53\u200b</p> <p>\u200b\u5220\u9664\u200b\u7ebf\u200b</p>"},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_9","title":"\u5206\u9694\u7ebf","text":""},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_10","title":"\u5f15\u7528","text":"<p>Stay hungry. Stay foolish.</p>"},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_11","title":"\u4ee3\u7801\u200b\u5757","text":"<p>\u200b\u4ee3\u7801\u200b\u5757\u200b\u652f\u6301\u200b\u6dfb\u52a0\u200b\u6807\u9898\u200b\u3001\u200b\u884c\u53f7\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u9ad8\u4eae\u200b\u90e8\u5206\u200b\u884c\u200b</p> hello_world.py<pre><code>print('Hello World!')\nprint('Hello World!')\nprint('Hello World!')\nprint('Hello World!')\n</code></pre>"},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_12","title":"\u6570\u5b66\u516c\u5f0f","text":"<p>\u200b\u8d28\u80fd\u200b\u65b9\u7a0b\u200b\\(E=mc^2\\)\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u6709\u540d\u200b\u7684\u200b\u516c\u5f0f\u200b\u3002</p> \\[ E=mc^2 \\]"},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_13","title":"\u94fe\u63a5","text":"<p>Markdown\u200b\u5b98\u65b9\u200b\u6559\u7a0b\u200b</p> <p>Material for MkDocs Reference</p>"},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#annotations","title":"Annotations","text":"<p>Steve Jobs founded Apple. (1)</p> <ol> <li>in 1976</li> </ol>"},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_14","title":"\u6587\u732e\u200b\u5f15\u7528","text":"<p>Transformer<sup>1</sup></p> <p>Transformer-based Models<sup>1</sup><sup>2</sup></p>"},{"location":"blog/2023/05/23/markdown-syntax-in-material-for-mkdocs/#_15","title":"\u56fe\u7247","text":"<p>\u200b\u652f\u6301\u200b\u6dfb\u52a0\u200b\u56fe\u7247\u200b\u6807\u9898\u200b</p> <p></p> <p>\u200b\u4e00\u53ea\u200b\u53ef\u7231\u200b\u7684\u200b\u732b\u732b\u200b</p> <p> </p> \u200b\u4e00\u53ea\u200b\u53ef\u7231\u200b\u7684\u200b\u732b\u732b\u200b <ol> <li> <p>Vaswani et al. Attention Is All You Need. (NIPS 2017)\u00a0\u21a9\u21a9</p> </li> <li> <p>Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (NAACL 2019)\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2022/07/20/%E9%9D%92%E6%B5%B7/","title":"\u9752\u6d77","text":"<p>2022\u200b\u5e74\u200b\u7684\u200b\u590f\u5929\u200b\u548c\u200b\u6bcd\u4eb2\u200b\u4e00\u8d77\u200b\u53bb\u200b\u4e86\u200b\u9752\u6d77\u200b\uff0c\u200b\u8349\u539f\u200b\u8352\u6f20\u200b\u7684\u200b\u5730\u8c8c\u200b\u5bf9\u4e8e\u200b\u6765\u81ea\u200b\u957f\u4e09\u89d2\u200b\u5730\u533a\u200b\u7684\u200b\u6211\u200b\u6765\u8bf4\u200b\u975e\u5e38\u200b\u65b0\u5947\u200b\u3002\u200b\u53ef\u60dc\u200b\u524d\u200b\u4e00\u534a\u200b\u65c5\u7a0b\u200b\u4e00\u76f4\u200b\u6709\u96e8\u200b\uff0c\u200b\u540e\u9762\u200b\u51e0\u5929\u200b\u624d\u200b\u89c1\u5230\u200b\u84dd\u5929\u200b\u3002</p> <p></p> <p>\u200b\u96c5\u4e39\u200b\u5730\u8c8c\u200b</p> <p></p> <p>\u200b\u76d0\u6e56\u200b</p> <p></p> <p>\u200b\u897f\u5317\u200b\u5927\u200b\u73af\u7ebf\u200b\u4e0a\u200b\u7684\u200b\u52a0\u6cb9\u7ad9\u200b</p> <p></p> <p>\u200b\u897f\u5317\u200b\u5927\u200b\u73af\u7ebf\u200b\u4e0a\u200b\u7684\u200b\u8def\u6807\u200b</p> <p></p> <p>\u200b\u7941\u8fde\u5c71\u200b</p>"},{"location":"blog/2023/01/29/%E6%9F%B4%E5%8F%AF%E5%A4%AB%E6%96%AF%E5%9F%BA%E5%86%99%E7%BB%99%E6%A2%85%E5%85%8B%E5%A4%AB%E4%BA%BA%E7%9A%84%E4%BF%A1/","title":"\u67f4\u53ef\u592b\u65af\u57fa\u200b\u5199\u7ed9\u200b\u6885\u514b\u592b\u4eba\u200b\u7684\u200b\u4fe1","text":"<p>\u200b\u67d0\u65e5\u200b\u5728\u200bTchaikovsky Research\u200b\u95f2\u901b\u200b\uff0c\u200b\u8bfb\u5230\u200b\u4e86\u200b1878\u200b\u5e74\u200b\u67f4\u53ef\u592b\u65af\u57fa\u200b\u5728\u200b\u4f5b\u7f57\u4f26\u8428\u200b\u5199\u7ed9\u200b\u6885\u514b\u592b\u4eba\u200b\u7684\u200b\u4fe1\u200b\uff0c\u200b\u597d\u200b\u611f\u52a8\u200b\u597d\u200b\u611f\u52a8\u200b\u3002\u200b\u67f4\u53ef\u592b\u65af\u57fa\u200b\u662f\u200b\u6211\u200b\u6700\u200b\u559c\u6b22\u200b\u7684\u200b\u53e4\u5178\u200b\u4f5c\u66f2\u5bb6\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u4e2a\u4eba\u200b\u89c9\u5f97\u200b\u9605\u8bfb\u200b\u8fd9\u200b\u5c01\u4fe1\u200b\u5bf9\u4e8e\u200b\u7406\u89e3\u200b\u67f4\u53ef\u592b\u65af\u57fa\u200b\u7684\u200b\u4e0d\u5c11\u200b\u4f5c\u54c1\u200b\u5f88\u200b\u6709\u200b\u5e2e\u52a9\u200b\u3002</p> <p>\u200b\u4fe1\u200b\u672c\u8eab\u200b\u662f\u200b\u8ba8\u8bba\u200b\u7b2c\u56db\u200b\u4ea4\u54cd\u66f2\u200b\u7684\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u201c\u200b\u6211\u4eec\u200b\u7684\u200b\u4ea4\u54cd\u66f2\u200b\u201d\u3002\u200b\u53e6\u5916\u200b\u4fe1\u200b\u7684\u200b\u7ed3\u5c3e\u5904\u200b\u67f4\u53ef\u592b\u65af\u57fa\u200b\u63d0\u5230\u200b\u4e86\u200b\u4ed6\u200b\u5373\u5c06\u200b\u524d\u5f80\u200b\u745e\u58eb\u200b\uff0c\u200b\u4e5f\u200b\u5c31\u662f\u200b\u4ed6\u200b\u5199\u4f5c\u200b\u5c0f\u63d0\u7434\u200b\u534f\u594f\u66f2\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p> <p>\u200b\u82f1\u6587\u7ffb\u8bd1\u200b\u5168\u6587\u200b\uff1a</p> <p><p>\\(Florence\\ \\frac{1\\ March}{17\\ February}\\ 1878\\)</p> How much joy your letter brought me today, my precious Nadezhda Filaretovna. How immeasurably happy I was that the symphony pleased you, that when hearing it you experienced the feelings with which I was suffused when I wrote it, and that the music sank into your heart.</p> <p>You asked me whether there is a definite programme to this symphony? Usually when this question is put to me about a symphonic work my answer is: none! Indeed, this is a difficult question to answer. How can one put into words the intangible sensations which one experiences when writing an instrumental work without a specific subject? This is a purely lyrical process. This is, fundamentally, an unburdening of the soul in music, with its essence distilled into sounds, in the same manner in which a lyrical poet expresses himself in verse. The only difference is that music has much more powerful means and a more subtle language with which to express thousands of different emotions and frames of mind. Usually the seed of a future work will manifest itself suddenly in unexpected ways. If the soil is fertile, i.e. if there is a disposition to work, the seed will take root with remarkable power and swiftness, allowing buds to emerge from the soil, followed by leaves, branches and, ultimately, flowers. I cannot define the creative process without resorting to metaphors. The difficulty lies in the fact that the seed requires favourable conditions in which to germinate. Everything else happens by itself. It would be futile for me to try to express to you in words the immeasurable bliss of all the feelings that seize me when a main idea appears, and when it begins to flourish into a particular form. I forget everything and become literally like a madman, everything within me shakes and pulses, with barely time to scribble out my sketches as one idea runs into another... Sometimes in the midst of this magical process, some external stimulus will jolt me out of this somnambuilistic state. Somebody might call, a servant enter, or a clock will strike and remind me that I need to go out on business... Such breaks are inexpressibly burdensome. Sometimes inspiration will fly away for quite a while. It's necessary to search for it, and often in vain. It is frequently necessary to fall back on an altogether cold, rational and technical working method. Perhaps it is because of this that the greatest masters have moments with an absence of organic flair, where the seams within the whole appear artificially sewn together. But it is impossible for it to be otherwise. If the condition of the artist's soul called inspiration that I am attempting to describe to you were to be continued without interruption, it should be impossible to live for a single day. The strings would snap, and the instrument should be dashed into smithereens! Only one thing is necessary: that the principal idea and the general outlines of all the movements did not come about by striving, but rather that they present themselves as a result of that supernatural, incomprehensible, and unfathomable force that is called inspiration.</p> <p>But I have digressed on an aside without answering your question. In our symphony there is a programme, i.e. it is possible to express in words what it is trying to say, and to you, and only to you, I am able and willing to explain the meaning both of the whole and of the separate movements. Of course, I can do this only in general terms.</p> <p>The introduction is the seed of the whole symphony, undoubtedly the main idea:</p> <p> </p> <p>This is Fate: this is that fateful force which prevents the impulse to happiness from attaining its goal, which jealously ensures that peace and happiness shall not be complete and unclouded, which hangs above the head like the sword of Damocles, unwaveringly, constantly poisoning the soul. It is an invincible force that can never be overcome \u2014 merely endured, hopelessly.</p> <p> </p> <p>The bleak and hopeless feelings grow stronger and intense. Is it not better to escape from reality and to immerse oneself in dreams:</p> <p> </p> <p>Oh joy! Out of nowhere a sweet and gentle day-dream appears. Some blissful, radiant human image hurries by and beckons us away:</p> <p> </p> <p>How wonderful! How distant the obsessive first theme of the allegro now sounds! Gradually the soul is enveloped by daydreams. Everything gloomy and joyless is forgotten. Here it is, here it is \u2014 happiness!</p> <p>No! These were daydreams, and Fate wakes us from them:</p> <p> </p> <p>And thus all life is an unbroken alternation of harsh reality with fleeting dreams and visions of happiness... No haven exists... Drift upon that sea until it engulfs and submerges you in its depths. That, roughly, is the programme of the first movement.</p> <p>The second movement of the symphony expresses another aspect of sadness. This is that melancholy feeling which comes in the evening when, weary from one's toil, one sits alone with a book \u2014 but it falls from the hand. There come a whole host of memories. It is sad that so much is now in the past, albeit pleasant to recall one's youth. Both regretting the past, and yet not wishing to begin life over again. Life is wearisome. It is pleasant to rest and look around. Memories abound! Happy moments when the young blood boiled, and life was satisfying. There are also painful memories, irreconcilable losses. All this is now somewhere far distant. It is both sad, yet somehow sweet to be immersed in the past...</p> <p>The third movement expresses no specific feeling. This is whimsical arabesques, vague images which can sweep past the imagination after drinking a little wine and feeling the first phases of intoxication. The spirit is neither cheerful, nor sad. Thinking about nothing in particular, giving free rein to the imagination, which somehow begins to paint strange pictures... Amid these memories there suddenly comes a picture of drunken peasants and a street song... Then, somewhere in the distance, a military procession passes. These are completely incoherent images which sweep through the head as one falls asleep. They have nothing in common with reality; they are strange, wild, and incoherent...</p> <p>The fourth movement. If within yourself you find no reasons for joy, then look at others. Go out among the people. See how they can enjoy themselves, surrendering themselves wholeheartedly to joyful feelings. Picture the festive merriment of ordinary people. Hardly have you managed to forget yourself and to be carried away by the spectacle of the joys of others, than irrepressible fate appears again and reminds you of yourself. But others do not care about you, and they have not noticed that you are solitary and sad. O, how they are enjoying themselves! How happy they are that all their feelings are simple and straightforward. Reproach yourself, and do not say that everything in this world is sad. Joy is a simple but powerful force. Rejoice in the rejoicing of others. To live is still possible.</p> <p>That, my dear friend, is all I can explain to you about the symphony. Of course, this is vague and incomplete. But an intrinsic quality of instrumental music is that it does not yield to detailed analysis. Where words end, music begins, as Heine remarked.</p> <p>It's already late. I'm not writing anything to you about Florence at this time, except that its very, very pleasant memories will stay with me for my whole life. At the end of next week, that is, around the 24<sup>th</sup> (by our style), I am thinking of going to Switzerland, where I intend to live quietly for the whole of March, gradually writing compositions in a variety of small forms. And so, when you receive this letter, my address shall once again be: Clarens, Canton de Vaud, Villa Richelieu.</p> <p>Thank you, my dear, for today's letter. I still have had no word from my Moscow friends. I will write to you about my opinion of them in detail.</p> <p><p>P. Tchaikovsky</p></p> <p>P. S. Just as I was about to put the letter in an envelope, I re-read it and was horrified at the incoherence and inadequacy of the programme I sent to you. This is the first time in my life that I have attempted to translate musical thoughts and images into words, and I could not manage to do this adequately. I was severely depressed last winter when writing the symphony, and it serves as a faithful echo of what I was experiencing. But it is known as an echo. How can it be translated into a clear and coherent succession of words? I do not know how to do that. I have already forgotten so much. They remain general recollections of the passions and mysterious feelings that I experienced. I am very, very curious about what my Moscow friends will say. Farewell.</p> <p><p>Yours P. Tchaikovsky</p>  Yesterday I spent the evening at the National Theatre and laughed a great deal. Italian comedy is vulgar, devoid of subtlety and grace, but immensely enjoyable. </p>"},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/category/%E6%8A%80%E6%9C%AF/","title":"\u6280\u672f","text":""},{"location":"blog/category/%E5%9F%BA%E7%A1%80/","title":"\u57fa\u7840","text":""},{"location":"blog/category/%E7%94%9F%E6%B4%BB/","title":"\u751f\u6d3b","text":""}]}